{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31916d03-61e2-47a7-b1de-392316730f4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Install Faker\n",
    "Pip install faker which is a pyhthon package that creates fake data based on the inputs you give. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5af989de-5941-49f7-9a38-ef4a5412cf50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6bd002a-3304-48c5-99d0-1e1cfa93ca53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "\n",
    "fake = Faker()\n",
    "data = [(fake.first_name(), fake.last_name(), fake.address(), fake.email(), fake.phone_number(), fake.date_of_birth()) for _ in range(50)]\n",
    "df = pd.DataFrame(data, columns=['first_name', 'last_name', 'address', 'email', 'phone_number', 'date_of_birth'])\n",
    "df['id'] = range(1, 51)\n",
    "\n",
    "spark_df = spark.createDataFrame(df)\n",
    "spark_df = spark_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "spark_df.write.mode(\"overwrite\").saveAsTable(\"main_sj.customers.customerdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "650dca69-1ddb-4df4-b3f7-40be24ab8462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "Select * from main_sj.customers.customerdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1386832e-4ec6-4068-90e8-ab237e66020c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Generate fake data\n",
    "data = [\n",
    "    (fake.word(), round(random.uniform(10, 1000), 2), random.randint(1, 10), fake.date_this_decade())\n",
    "    for _ in range(100)\n",
    "]\n",
    "orders_df = pd.DataFrame(data, columns=['article_name', 'unit_price', 'amount', 'purchase_date'])\n",
    "\n",
    "# Calculate total_sum\n",
    "orders_df['total_sum'] = orders_df['unit_price'] * orders_df['amount']\n",
    "\n",
    "# Add customer_id from existing customer data\n",
    "customer_ids = spark.sql(\"SELECT id FROM main_sj.customers.customerdata\").rdd.flatMap(lambda x: x).collect()\n",
    "orders_df['customer_id'] = [random.choice(customer_ids) for _ in range(100)]\n",
    "\n",
    "# Add id column\n",
    "orders_df['id'] = range(1, 101)\n",
    "\n",
    "# Create Spark DataFrame\n",
    "spark_orders_df = spark.createDataFrame(orders_df)\n",
    "\n",
    "# Write to external table\n",
    "spark_orders_df.write.mode(\"overwrite\").saveAsTable(\"main_sj.customers.orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e7a3b17-0135-4862-8fab-e0aed1b79606",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, rand\n",
    "import random\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "\n",
    "fake= Faker()\n",
    "\n",
    "data = [(i, fake.word(), round(random.uniform(10, 100), 2), random.randint(1, 10), 0.0, fake.date_this_year(), random.randint(1, 50)) for i in range(1, 101)]\n",
    "df = pd.DataFrame(data, columns=['order_id', 'article_name', 'unit_price', 'amount', 'total_price', 'purchase_date', 'customer_id'])\n",
    "df['total_price'] = df['unit_price'] * df['amount']\n",
    "\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "spark_df.write.mode(\"overwrite\").saveAsTable(\"main_sj.customers.orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62509032-e61c-49d7-af75-20e6132170b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4389956761189825,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Create data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
