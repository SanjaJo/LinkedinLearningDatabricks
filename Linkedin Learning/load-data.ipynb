{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0093eb1-e9d3-4c28-86c0-e6db58275c04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### setup\n",
    "1. use the dataset available at open datasets at microsoft https://learn.microsoft.com/de-de/azure/open-datasets/dataset-taxi-yellow?tabs=pyspark \n",
    "2. save the first 2000 rows as table to unity catalog\n",
    "3. calcuate the trip duration \n",
    "4. save the cleaned data as a new table to be used for auto ml training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efa55120-860c-4894-bfb4-249cde42941e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Azure storage access info\n",
    "blob_account_name = \"azureopendatastorage\"\n",
    "blob_container_name = \"nyctlc\"\n",
    "blob_relative_path = \"yellow\"\n",
    "blob_sas_token = \"r\"\n",
    "\n",
    "# Allow SPARK to read from Blob remotely\n",
    "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
    "spark.conf.set(\n",
    "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
    "  blob_sas_token)\n",
    "print('Remote blob path: ' + wasbs_path)\n",
    "\n",
    "# SPARK read parquet, note that it won't load any data yet by now\n",
    "df = spark.read.parquet(wasbs_path)\n",
    "print('Register the DataFrame as a SQL temporary view: source')\n",
    "df.createOrReplaceTempView('source')\n",
    "\n",
    "# Display top 10 rows\n",
    "print('Displaying top 10 rows: ')\n",
    "display(spark.sql('SELECT * FROM source LIMIT 10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96365bc6-28d2-4361-8590-19bf4a7d3900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.limit(2000).write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"main_sj.linkedinlearning.taxicomplete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df519295-0bb7-4a43-ae61-e7017ec307ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate trip duration\n",
    "df_trip_duration = spark.sql(\"\"\"\n",
    "SELECT *,\n",
    "       (unix_timestamp(tpepDropoffDateTime) - unix_timestamp(tpepPickupDateTime)) / 60 AS trip_duration_minutes\n",
    "FROM main_sj.linkedinlearning.taxicomplete\n",
    "\"\"\")\n",
    "\n",
    "# Display the result\n",
    "display(df_trip_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfe00857-aa5e-408c-824a-6d4159bc1a64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_trip_duration.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"main_sj.linkedinlearning.taxicomplete_trip_duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44239c89-1f8d-44f6-b01f-3ca4f80817a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned = spark.sql(\"\"\"\n",
    "SELECT vendorID, passengerCount, tripDistance, paymentType, fareAmount, extra, mtaTax, tipAmount, tollsAmount, trip_duration_minutes\n",
    "FROM main_sj.linkedinlearning.taxicomplete_trip_duration\n",
    "\"\"\")\n",
    "\n",
    "df_cleaned.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"main_sj.linkedinlearning.cleaneddata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e404e9cc-e59c-41a6-8649-9480cd3afa5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "load-data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
